# Default values for catalog-server.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- Deploy with settings for development (e.g., num_brokers = 1)
dev_deployment: false

# -- Number of catalog-server replicas to deploy
replicaCount: 1

image:
  # -- Image registry
  registry: docker.io
  # -- Image repository
  repository: radarbase/radar-schemas-tools
  # -- Image tag (immutable tags are recommended)
  # Overrides the image tag whose default is the chart appVersion.
  tag:
  # -- Image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  digest: ""
  # -- Image pull policy
  pullPolicy: IfNotPresent
  # -- Optionally specify an array of imagePullSecrets.
  # Secrets must be manually created in the namespace.
  # e.g:
  # pullSecrets:
  #   - myRegistryKeySecretName
  #
  pullSecrets: []

# -- String to partially override catalog-server.fullname template with a string (will prepend the release name)
nameOverride: ""
# -- String to fully override catalog-server.fullname template with a string
fullnameOverride: ""

# -- Configure catalog-server pods' Security Context
podSecurityContext:
  fsGroup: 101

# -- Configure Appconfig containers' Security Context
securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  # -- Kubernetes Service type
  type: ClusterIP
  # -- catalog-server port
  port: 9010

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # -- CPU/Memory resource limits
  # limits:
  #   cpu: 200m
  #   memory: 512Mi
  # -- CPU/Memory resource requests
  requests:
    cpu: 100m
    memory: 256Mi

persistence:
  # -- Enable persistence using PVC
  enabled: true
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass: "-"
  ##
  ## If you want to reuse an existing claim, you can pass the name of the PVC using
  ## the existingClaim variable
  # existingClaim: your-claim
  # -- PVC Access Mode for catalog-server volume
  accessMode: ReadWriteOnce
  # -- PVC Storage Request for catalog-server volume
  size: 5Mi

# -- Node labels for pod assignment
nodeSelector: {}

# -- Toleration labels for pod assignment
tolerations: []

# -- Affinity labels for pod assignment
affinity: {}

# -- Extra environment variables
extraEnvVarsInit: []
#  - name: BEARER_AUTH
#    value: true

# -- Custom livenessProbe that overrides the default one
customLivenessProbe: {}

livenessProbe:
  # -- Enable livenessProbe
  enabled: true
  # -- Initial delay seconds for livenessProbe
  initialDelaySeconds: 5
  # -- Period seconds for livenessProbe
  periodSeconds: 30
  # -- Timeout seconds for livenessProbe
  timeoutSeconds: 5
  # -- Success threshold for livenessProbe
  successThreshold: 1
  # -- Failure threshold for livenessProbe
  failureThreshold: 3

# -- Custom readinessProbe that overrides the default one
customReadinessProbe: {}

readinessProbe:
  # -- Enable readinessProbe
  enabled: true
  # -- Initial delay seconds for readinessProbe
  initialDelaySeconds: 5
  # -- Period seconds for readinessProbe
  periodSeconds: 30
  # -- Timeout seconds for readinessProbe
  timeoutSeconds: 5
  # -- Success threshold for readinessProbe
  successThreshold: 1
  # -- Failure threshold for readinessProbe
  failureThreshold: 3

# -- Custom startupProbe that overrides the default one
customStartupProbe: {}

startupProbe:
  # -- Enable startupProbe
  enabled: true
  # -- Initial delay seconds for startupProbe
  initialDelaySeconds: 5
  # -- Period seconds for startupProbe
  periodSeconds: 10
  # -- Timeout seconds for startupProbe
  timeoutSeconds: 10
  # -- Success threshold for startupProbe
  successThreshold: 1
  # -- Failure threshold for startupProbe
  failureThreshold: 30

# -- Network policy defines who can access this application and who this applications has access to
# @default -- check `values.yaml`
networkpolicy:
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: '{{ .Release.Namespace }}'
      podSelector:
        matchLabels:
          app.kubernetes.io/name: management-portal
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: '{{ .Release.Namespace }}'
      podSelector:
        matchLabels:
          app.kubernetes.io/name: radar-s3-connector
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8
        - 192.168.0.0/16
        - 172.16.0.0/12
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
      - port: 53
        protocol: UDP
      - port: 53
        protocol: TCP
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: '{{ .Release.Namespace }}'
      podSelector:
        matchLabels:
          app.kubernetes.io/name: radar-kafka-bootstrap
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: '{{ .Release.Namespace }}'
      podSelector:
        matchLabels:
          app.kubernetes.io/name: radar-schema-registry

# -- Number of deployed Kafka brokers
kafka_num_brokers: 3
# -- Number of Kafka Topic replicates. Should be min 2, max 4, and is normally 3, but is limited by the number of brokers.
# ref: https://learn.conduktor.io/kafka/kafka-topics-choosing-the-replication-factor-and-partitions-count/
# -1 sets to use default configured in the kafka broker.
kafka_num_replication: -1
# -- Number of Kafka Topic data partitions, Rule of thumb 3 times the number of brokers. Headroom is used for future upscale of brokers.
# ref: https://learn.conduktor.io/kafka/kafka-topics-choosing-the-replication-factor-and-partitions-count/
# -1 sets to use default configured in the kafka broker.
kafka_num_partitions: -1
# -- URI of Kafka brokers
# On strimzi operator, this points to the listener that has no tls, and scram-sha-512 authentication.
kafka: radar-kafka-bootstrap:9094
# -- URL of the confluent schema registry
schema_registry: http://radar-schema-registry:8081

# -- Additional kafka properties such as security config. The template replaces `_` with `.` in keys so property keys can be specified using `_` instead of `.`. For example `security_protocol` is same as `security.protocol` kafka config.
kafkaProperties:
# -- Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
  security_protocol: SASL_PLAINTEXT
  # -- Mechanism used to authenticate with SASL. Valid values are: PLAIN, SCRAM-SHA-256, SCRAM-SHA-512.
  sasl_mechanism: SCRAM-SHA-512

secret:
  # -- Secret for the Kafka SASL JAAS configuration
  jaas:
    name: shared-service-user
    key: sasl.jaas.config

sources:
  # -- Only include given specification directory files. You can use File glob syntax as described in <https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystem.html#getPathMatcher-java.lang.String-> If include is specified, exclude will be ignored. The glob pattern should start from the specifications directory.
  include: []
  # -- Exclude all given specification directory files. You can use File glob syntax as described in <https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystem.html#getPathMatcher-java.lang.String-> If include is specified, exclude will be ignored. The glob pattern should start from the specifications directory.
  exclude: []
    # - active/*
    # - passive/*1.0.0*
  # -- active source specification, as done in RADAR-schemas/specifications/active. The array elements should be the full YAML specification.
  active: []
  # -- connector source specification, as done in RADAR-schemas/specifications/connector. The array elements should be the full YAML specification.
  connector: []
  # -- monitor source specification, as done in RADAR-schemas/specifications/monitor. The array elements should be the full YAML specification.
  monitor: []
  # -- passive source specification, as done in RADAR-schemas/specifications/passive. The array elements should be the full YAML specification.
  passive: []
  # -- push source specification, as done in RADAR-schemas/specifications/push. The array elements should be the full YAML specification.
  push: []
  # -- stream source specification, as done in RADAR-schemas/specifications/stream. The array elements should be the full YAML specification.
  stream: []

schemas:
  # -- Only include given schema directory files. You can use File glob syntax as described in <https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystem.html#getPathMatcher-java.lang.String->. If include is specified, exclude will be ignored. The glob pattern should start from the commons directory.
  include: []
  # -- Exclude all given schema directory files. You can use File glob syntax as described in <https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystem.html#getPathMatcher-java.lang.String->. If include is specified, exclude will be ignored. The glob pattern should start from the commons directory.
  exclude: []
    # - active/*
    # - passive/*1.0.0*
  # -- active record schemas, as done in RADAR-schemas/commons/active. The object fields should be the file name, e.g. `application/application_uptime.avsc`.
  active: {}
  # -- catalogue record schemas, as done in RADAR-schemas/commons/catalogue. The object fields should be the file name, e.g. `application/application_uptime.avsc`.
  catalogue: {}
  # -- connector record schemas, as done in RADAR-schemas/commons/connector. The object fields should be the file name, e.g. `application/application_uptime.avsc`.
  connector: {}
  # -- catalogue record schemas, as done in RADAR-schemas/commons/kafka. The object fields should be the file name, e.g. `application/application_uptime.avsc`.
  kafka: {}
  # -- monitor record schemas, as done in RADAR-schemas/commons/monitor. The object fields should be the file name, e.g. `application/application_uptime.avsc`.
  monitor: {}
  # -- passive record schemas, as done in RADAR-schemas/commons/passive. The object fields should be the file name, e.g. `application/application_uptime.avsc`.
  passive: {}
  # -- push record schemas, as done in RADAR-schemas/commons/push. The object fields should be the file name, e.g. `application/application_uptime.avsc`.
  push: {}
  # -- stream record schemas, as done in RADAR-schemas/commons/stream. The object fields should be the file name, e.g. `application/application_uptime.avsc`.
  stream: {}

cc:
  # -- set to true if using Confluent Cloud for kafka cluster and schema registry
  enabled: false
  # -- URL of the bootstrap server of Confluent Cloud based kafka cluster
  bootstrapServerurl: confluent-url
  # -- API key of the Confluent Cloud based kafka cluster
  apiKey: ccApikey
  # -- API secret of the Confluent Cloud based kafka cluster
  apiSecret: ccApiSecret
  # -- API key of the Confluent Cloud based schema registry
  schemaRegistryApiKey: srApiKey
  # -- API secret of the Confluent Cloud based schema registry
  schemaRegistryApiSecret: srApiSecret
