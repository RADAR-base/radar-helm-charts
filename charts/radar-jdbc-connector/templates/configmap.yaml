{{- include "radar-jdbc-connector.validateValues" . }}
{{- $isSource := eq .Values.mode "source" -}}
{{- $name := ternary  .Values.source.name .Values.sink.name $isSource -}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "common.names.fullname" . }}
  namespace: {{ include "common.names.namespace" . | quote }}
  labels: {{- include "common.labels.standard" ( dict "customLabels" .Values.commonLabels "context" $ ) | nindent 4 }}
  {{- if .Values.commonAnnotations }}
  annotations: {{- include "common.tplvalues.render" ( dict "value" .Values.commonAnnotations "context" $ ) | nindent 4 }}
  {{- end }}
data:
  connector.properties: |
    # Kafka connector configuration
    name = {{ $name }}

    tasks.max = {{ .Values.maxTasks }}

    # General connection parameters
    {{/*At moment of this writing, 'connection.url' cannot be set from environment variable with JDBC-connector.
     As such, it has to be added in config file and is created when any URL secret has been created*/}}
    connection.url = {{ include "radar-jdbc-connector.databaseUrl" . }}
    jdbc.credentials.provider.class=io.confluent.connect.jdbc.util.EnvVarsJdbcCredentialsProvider
    dialect.name = {{ .Values.jdbc.dialect }}
    max.retries = 15
    # Wait 10 minutes before retrying failed task
    retry.backoff.ms = 600000
    connection.attempts = 15
    # Wait 10 minutes before attempting connection
    connection.backoff.ms = 600000

    {{- if not $isSource }}
    # Kafka connector configuration
    connector.class = io.confluent.connect.jdbc.JdbcSinkConnector

    {{- with .Values.sink }}
    insert.mode = {{ .insertMode }}
    # Topics that will be consumed
    topics = {{ .topics }}
    table.name.format = {{ .tableNameFormat }}
    auto.create = {{ .autoCreate }}
    {{- with .primaryKeys }}
    pk.mode = {{ .mode }}
    pk.fields = {{ join ", " .fields }}
    {{- end }}

    {{- if or (dig "mergeKey" false .) (dig "transforms" "enabled" false .) }}
    {{- with .transforms }}
    transforms=mergeKey,timestamp
    transforms.mergeKey.type={{ .mergeKeyType }}
    transforms.timestamp.type={{ .timestampType }}
    transforms.timestamp.fields={{ join ", " .timestampFields }}
    {{- end }}
    {{- end }}
    {{- end }}
    {{- end }}

    {{- if $isSource }}
    # Kafka connector configuration
    connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
    {{- with .Values.source }}
    {{- if .schema }}
    schema.pattern = {{ .schema }}
    {{- end }}
    table.whitelist = {{ .tableWhitelist }}
    topic.prefix = {{ .topicPrefix }}
    mode = {{ .mode }}
    incrementing.column.name = {{ .incrementingColumnName }}
    {{- if .keyField }}
    transforms=createKey,extractField
    transforms.createKey.type = org.apache.kafka.connect.transforms.ValueToKey
    transforms.createKey.fields = {{ .keyField }}
    transforms.extractField.type = org.apache.kafka.connect.transforms.ExtractField$Key
    transforms.extractField.field = {{ .keyField }}
    {{- end }}
    {{- end }}
    {{/*-1 means use default configured by the kafka broker.
     Ref: https://docs.confluent.io/platform/7.9/connect/references/allconfigs.html#distributed-worker-configuration*/}}
    topic.creation.default.replication.factor = -1
    topic.creation.default.partitions = -1
    {{- end }}

    key.converter = io.confluent.connect.avro.AvroConverter
    key.converter.schema.registry.url = {{ .Values.schema_registry }}
    value.converter = io.confluent.connect.avro.AvroConverter
    value.converter.schema.registry.url = {{ .Values.schema_registry }}
    errors.log.enable = true
    errors.log.include.messages = true
    errors.tolerance = all
  healthcheck.sh: |
    #!/bin/sh
    STATUS=$(curl -sf localhost:8083/connectors/{{ $name }}/status | grep -o '\"state\":\"[^\"]*\"')
    if echo "$STATUS" | tr '\\n' ',' | grep -q FAILED; then
      exit 1
    elif [ $(echo "$STATUS" | grep RUNNING | wc -l) -le 1 ]; then
      exit 1
    fi
