# Default values for radar-jdbc-connector.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- Number of radar-jdbc-connector replicas to deploy
replicaCount: 1

image:
  # -- Image registry
  registry: ghcr.io
  # -- Image repository
  repository: radar-base/radar-jdbc-connector
  # -- Image tag (immutable tags are recommended)
  # Overrides the image tag whose default is the chart appVersion.
  tag:
  # -- Image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  digest: ""
  # -- Image pull policy
  pullPolicy: IfNotPresent
  # -- Optionally specify an array of imagePullSecrets.
  # Secrets must be manually created in the namespace.
  # e.g:
  # pullSecrets:
  #   - myRegistryKeySecretName
  #
  pullSecrets: []

# -- String to partially override radar-jdbc-connector.fullname template with a string (will prepend the release name)
nameOverride: ""
# -- String to fully override radar-jdbc-connector.fullname template with a string
fullnameOverride: ""

# -- Configure radar-jdbc-connector pods' Security Context
podSecurityContext: {}
  # fsGroup: 2000

# -- Configure radar-jdbc-connector containers' Security Context
securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  # -- Kubernetes Service type
  type: ClusterIP
  # -- radar-jdbc-connector port
  port: 8083

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi

  # -- CPU/Memory resource requests
  requests:
    cpu: 100m
    memory: 1Gi

# -- Node labels for pod assignment
nodeSelector: {}

# -- Toleration labels for pod assignment
tolerations: []

# -- Affinity labels for pod assignment
affinity: {}

# -- Additional environment variables to pass to the connector. These can be used to pass supported kafka and connect specific [configs](https://docs.confluent.io/platform/current/installation/docker/config-reference.html#kconnect-long-configuration)
extraEnvVars:
  # -- Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
  # Needed by kafka_init script.
  - name: CONNECT_SECURITY_PROTOCOL
    value: SASL_PLAINTEXT
  # -- Mechanism used to authenticate with SASL. Valid values are: PLAIN, SCRAM-SHA-256, SCRAM-SHA-512.
  # Needed by kafka_init script.
  - name: CONNECT_SASL_MECHANISM
    value: SCRAM-SHA-512
  # -- Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
  - name: CONNECT_CONSUMER_SECURITY_PROTOCOL
    value: SASL_PLAINTEXT
  # -- Mechanism used to authenticate with SASL. Valid values are: PLAIN, SCRAM-SHA-256, SCRAM-SHA-512.
  - name: CONNECT_CONSUMER_SASL_MECHANISM
    value: SCRAM-SHA-512

secret:
  # -- Secret name for the JAAS configuration
  jaas:
    name: shared-service-user
    key: sasl.jaas.config

# -- Custom livenessProbe that overrides the default one
customLivenessProbe: {}

livenessProbe:
  # -- Enable livenessProbe
  enabled: true
  # -- Initial delay seconds for livenessProbe
  initialDelaySeconds: 5
  # -- Period seconds for livenessProbe
  periodSeconds: 30
  # -- Timeout seconds for livenessProbe
  timeoutSeconds: 5
  # -- Success threshold for livenessProbe
  successThreshold: 1
  # -- Failure threshold for livenessProbe
  failureThreshold: 3

# -- Custom readinessProbe that overrides the default one
customReadinessProbe: {}

readinessProbe:
  # -- Enable readinessProbe
  enabled: true
  # -- Initial delay seconds for readinessProbe
  initialDelaySeconds: 5
  # -- Period seconds for readinessProbe
  periodSeconds: 30
  # -- Timeout seconds for readinessProbe
  timeoutSeconds: 5
  # -- Success threshold for readinessProbe
  successThreshold: 1
  # -- Failure threshold for readinessProbe
  failureThreshold: 3

# -- Custom startupProbe that overrides the default one
customStartupProbe: {}

startupProbe:
  # -- Enable startupProbe
  enabled: true
  # -- Initial delay seconds for startupProbe
  initialDelaySeconds: 5
  # -- Period seconds for startupProbe
  periodSeconds: 10
  # -- Timeout seconds for startupProbe
  timeoutSeconds: 10
  # -- Success threshold for startupProbe
  successThreshold: 1
  # -- Failure threshold for startupProbe
  failureThreshold: 30

# -- Network policy defines who can access this application and who this applications has access to
# @default -- check `values.yaml`
networkpolicy:
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: '{{ .Release.Namespace }}'
      podSelector:
        matchLabels:
          app.kubernetes.io/name: 'radar-kafka-kafka-bootstrap'
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: '{{ .Release.Namespace }}'
      podSelector:
        matchLabels:
          app.kubernetes.io/name: 'radar-kafka-schema-registry'
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: '{{ .Release.Namespace }}'
      podSelector:
        matchLabels:
          app.kubernetes.io/name: 'timescaledb-postgresql'
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
      - port: 53
        protocol: UDP
      - port: 53
        protocol: TCP

# -- URI of Kafka brokers of the cluster
kafka: SASL_PLAINTEXT://radar-kafka-kafka-bootstrap:9094
# -- Number of Kafka brokers. This is used to validate the cluster availability at connector init.
kafka_num_brokers: "3"
# -- URL of the Kafka schema registry
schema_registry: http://radar-kafka-schema-registry:8081

# -- Maximum number of worker threads inside a connector pod.
maxTasks: 2

# -- Either source or sink
mode: sink

logLevel:
  # -- Default log level
  root: INFO
  # -- Per-logger log-level
  loggers:
    org.reflections: ERROR

# -- Java heap options
heapOpts: "-Xms1500m"

source:
  # -- Name of the connector Kafka producer group
  name: radar-jdbc-source
  # -- Database schema (if any)
  schema: public
  # -- Comma-separated list of tables to read
  tableWhitelist: ""
  # -- Prefix to prepend to table names to generate the name of the Kafka topic to publish data to.
  topicPrefix: ""
  # -- How to detect new values in a table.
  mode: incrementing
  # -- When using mode incrementing, which column to use as incrementing. If empty, autodetection will be used.
  incrementingColumnName: ""
  # -- Field to use as key for the records. If empty, no key is used.
  keyField: ""
  persistence:
    # -- Whether to enable persistence for storing offsets
    enabled: true
    # -- Existing persistent volume claim to use
    existingClaim:
    # -- PVC access mode
    accessMode: ReadWriteOnce
    # -- PVC storage size request
    size: 20Mi

sink:
  # -- Name of the connector Kafka consumer group
  name: radar-jdbc-sink
  # -- create table if it does not exist
  autoCreate: true
  # -- How to insert new values into the database
  insertMode: upsert
  transforms:
    # -- Whether to merge the key fields into the inserted values.
    enabled: true
    # -- Class to handle merging the key into the record
    mergeKeyType: org.radarbase.kafka.connect.transforms.MergeKey
    # -- Class to convert time-based fields into SQL timestamps
    timestampType: org.radarbase.kafka.connect.transforms.TimestampConverter
    # -- Field names to consider for converting to timestamp
    timestampFields:
      - time
      - timeReceived
      - timeCompleted
      - timestamp

  primaryKeys:
    # -- where to read the primary keys from when creating the table
    mode: record_value
    # -- fields to include as primary keys when creating the table
    fields:
      - time
      - userId
      - projectId
  # -- Comma-separated list of topics the connector will read from and ingest into the database
  topics: android_phone_relative_location, android_phone_battery_level, connect_upload_altoida_summary, connect_fitbit_intraday_heart_rate, connect_fitbit_intraday_steps
  # -- How to format a table name based on the inserted topic
  tableNameFormat: "${topic}"

jdbc:
  # -- Host of the TimescaleDB database
  url:
  # -- Kubernetes secret name for the JDBC connection URL (disables the use of 'url' value)
  urlSecret:
    name:
    key:
  # -- TimescaleDB database username
  user:
  # -- Kubernetes secret name for the username (disables the use of 'user' value)
  userSecret:
    name:
    key:
  # -- TimescaleDB database password (using a secret is recommended)
  password:
  # -- Kubernetes secret name for the password
  passwordSecret:
    name:
    key:
  # -- JDBC connect dialect that the database uses
  dialect: TimescaleDBDatabaseDialect

timescaledb:
  # -- Use the local cloudnativepg timescaledb cluster
  enabled: true
  # -- CloudNativePG TimescaleDB configuration
  # @default -- check `values.yaml`
  cluster:
    fullnameOverride: jdbc-connector-timescaledb
    mode: standalone
    cluster:
      instances: 1
      initdb:
        database: database_name
        owner: database_name
